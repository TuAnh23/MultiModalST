{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import tarfile\n",
    "import zipfile\n",
    "import glob\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "# Change the working directory to the root of the project\n",
    "os.chdir(r'C:\\Users\\TuAhnDinh\\Desktop\\MediaanProjects\\BachelorThesisST')\n",
    "\n",
    "COVOST_DIR = 'data/CoVoST2'\n",
    "\n",
    "# Downloads voice clips and transcripts\n",
    "urls = {'en': 'https://voice-prod-bundler-ee1969a6ce8178826482b88e843c335139bd3fb4.s3.amazonaws.com/cv-corpus-4-2019-12-10/en.tar.gz',\n",
    "        'fr': 'https://voice-prod-bundler-ee1969a6ce8178826482b88e843c335139bd3fb4.s3.amazonaws.com/cv-corpus-4-2019-12-10/fr.tar.gz',\n",
    "        'de': 'https://voice-prod-bundler-ee1969a6ce8178826482b88e843c335139bd3fb4.s3.amazonaws.com/cv-corpus-4-2019-12-10/de.tar.gz',\n",
    "        'it': 'https://voice-prod-bundler-ee1969a6ce8178826482b88e843c335139bd3fb4.s3.amazonaws.com/cv-corpus-4-2019-12-10/it.tar.gz',\n",
    "        'pt': 'https://voice-prod-bundler-ee1969a6ce8178826482b88e843c335139bd3fb4.s3.amazonaws.com/cv-corpus-4-2019-12-10/pt.tar.gz',\n",
    "        'es': 'https://voice-prod-bundler-ee1969a6ce8178826482b88e843c335139bd3fb4.s3.amazonaws.com/cv-corpus-4-2019-12-10/es.tar.gz',\n",
    "        'nl': 'https://voice-prod-bundler-ee1969a6ce8178826482b88e843c335139bd3fb4.s3.amazonaws.com/cv-corpus-4-2019-12-10/nl.tar.gz'}\n",
    "\n",
    "for lang, url in urls.items():\n",
    "    lang_dir = COVOST_DIR + '/' + lang\n",
    "    if not os.path.exists(lang_dir):\n",
    "        print(f'Downloading {lang} audios')\n",
    "        filename = url.rsplit('/', 1)[1]\n",
    "        r = requests.get(url)\n",
    "        with open(COVOST_DIR + '/' + filename, 'wb') as f:\n",
    "            f.write(r.content)\n",
    "        print(f'Extracting {lang} audios')\n",
    "        tf = tarfile.open(COVOST_DIR + '/' + filename)\n",
    "        tf.extractall(lang_dir)\n",
    "        tf.close()\n",
    "        os.remove(COVOST_DIR + '/' + filename)\n",
    "\n",
    "XX_EN_LANGUAGES  = ['fr', 'de', 'it', 'pt', 'es', 'nl']\n",
    "EN_XX_LANGUAGES  = ['de', 'et']\n",
    "# Download CoVoST 2 translations (covost_v2.<src_lang_code>_<tgt_lang_code>.tsv, \n",
    "# which matches the rows in validated.tsv from Common Voice)\n",
    "if not os.path.exists(COVOST_DIR + '/covost2'):\n",
    "    os.mkdir(COVOST_DIR + '/covost2')\n",
    "for lang in XX_EN_LANGUAGES:\n",
    "    if not os.path.exists(COVOST_DIR + '/covost2' + f'/{lang}_en'):\n",
    "        os.mkdir(COVOST_DIR + '/covost2'+ f'/{lang}_en')\n",
    "        # Download and extract .tsv file\n",
    "        url = f'https://dl.fbaipublicfiles.com/covost/covost_v2.{lang}_en.tsv.tar.gz'\n",
    "        filename = url.rsplit('/', 1)[1]\n",
    "        print(f'Download and extracting {filename}')\n",
    "        r = requests.get(url)\n",
    "        with open(COVOST_DIR + '/covost2' + f'/{lang}_en' + f'/{filename}', 'wb') as f:\n",
    "            f.write(r.content)\n",
    "        tf = tarfile.open(COVOST_DIR + '/covost2' + f'/{lang}_en' + f'/{filename}')\n",
    "        tf.extractall(COVOST_DIR + '/covost2' + f'/{lang}_en')\n",
    "        tf.close()\n",
    "        os.remove(COVOST_DIR + '/covost2' + f'/{lang}_en' + f'/{filename}')\n",
    "        \n",
    "        # Split .tsv file into train, dev and test set\n",
    "        os.system(f\"python get_covost_splits.py \"\n",
    "                  f\"--version 2 --src-lang {lang} --tgt-lang en \"\n",
    "                  f\"--root {COVOST_DIR + '/covost2' + f'/{lang}_en'} \"\n",
    "                  f\"--cv-tsv {COVOST_DIR + '/' + lang + '/validated.tsv'}\")\n",
    "        \n",
    "for lang in EN_XX_LANGUAGES:\n",
    "    if not os.path.exists(COVOST_DIR + '/covost2' + f'/en_{lang}'):\n",
    "        os.mkdir(COVOST_DIR + '/covost2'+ f'/en_{lang}')\n",
    "        # Download and extract .tsv file\n",
    "        url = f'https://dl.fbaipublicfiles.com/covost/covost_v2.en_{lang}.tsv.tar.gz'\n",
    "        filename = url.rsplit('/', 1)[1]\n",
    "        print(f'Download and extracting {filename}')\n",
    "        r = requests.get(url)\n",
    "        with open(COVOST_DIR + '/covost2' + f'/en_{lang}' + f'/{filename}', 'wb') as f:\n",
    "            f.write(r.content)\n",
    "        tf = tarfile.open(COVOST_DIR + '/covost2' + f'/en_{lang}' + f'/{filename}')\n",
    "        tf.extractall(COVOST_DIR + '/covost2' + f'/en_{lang}')\n",
    "        tf.close()\n",
    "        os.remove(COVOST_DIR + '/covost2' + f'/en_{lang}' + f'/{filename}')\n",
    "        \n",
    "        # Split .tsv file into train, dev and test set\n",
    "        os.system(f\"python get_covost_splits.py \"\n",
    "                  f\"--version 2 --src-lang en --tgt-lang {lang} \"\n",
    "                  f\"--root {COVOST_DIR + '/covost2' + f'/en_{lang}'} \"\n",
    "                  f\"--cv-tsv {COVOST_DIR + '/' + 'en' + '/validated.tsv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note on data split**: \n",
    "\n",
    "- Use standard Common Voice dev/test splits (no duplicated sentences)\n",
    "- Use extended Common Voice train split to improve data utilization (include all duplicated sentences with different speakers)\n",
    "\n",
    "More info on the [Covost2 paper](https://arxiv.org/pdf/2007.10310.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A function to remove empty .mp3 audio lines from a split dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_empty_audio(split_df, audiodir):\n",
    "    empty = []\n",
    "    paths = split_df['path'].values\n",
    "    for path in paths:\n",
    "        if os.path.getsize(audiodir + '/' + path) == 0:\n",
    "            print(f\"found {path} to be empty\")\n",
    "            empty.append(path)\n",
    "    new_df = split_df.set_index('path')\n",
    "    new_df.drop(labels=empty, axis='index', inplace=True)\n",
    "    new_df.reset_index(inplace=True)\n",
    "    return new_df\n",
    "\n",
    "def remove_empty_transcription(split_df):\n",
    "    new_df = split_df.loc[(split_df['sentence'] != \"\") & (split_df['sentence'] != '\"\"') & (split_df['translation'] != \"\") & (split_df['translation'] != '\"\"')]\n",
    "    return new_df\n",
    "    \n",
    "def read_tsv_split(translation_dir, src_lang, tgt_lang, split, audiodir):\n",
    "    split_df = pd.read_csv(translation_dir + f'/covost_v2.{src_lang}_{tgt_lang}.{split}.tsv', sep='\\t', header=0, encoding=\"utf-8\", escapechar=\"\\\\\", quoting=csv.QUOTE_NONE, na_filter=False)\n",
    "    return remove_empty_transcription(remove_empty_audio(split_df, audiodir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take a look at Dutch --> English as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DUTCH_AUDIO_DIR = COVOST_DIR + '/nl'\n",
    "NL_EN_TRANSLATIONS_DIR = COVOST_DIR + '/covost2' + '/nl_en'\n",
    "nl_en_translations_dev = pd.read_csv(NL_EN_TRANSLATIONS_DIR + '/covost_v2.nl_en.dev.tsv', sep='\\t', header=0, encoding=\"utf-8\", escapechar=\"\\\\\", quoting=csv.QUOTE_NONE, na_filter=False)\n",
    "nl_en_translations_dev.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show some samples"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "n_samples = 3\n",
    "for i in range (0, n_samples):\n",
    "    print('- Audio:')\n",
    "    audio_path = DUTCH_AUDIO_DIR + '/clips/' + nl_en_translations_dev.loc[i]['path']\n",
    "    print(audio_path)\n",
    "    display(Audio(filename=audio_path, autoplay=False))\n",
    "    print('- Transcription - nl:')\n",
    "    print(nl_en_translations_dev.loc[i]['sentence'])\n",
    "    print('- Translated transcription - en:')\n",
    "    print(nl_en_translations_dev.loc[i]['translation'])\n",
    "    print('-------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">numb. of audios</th>\n",
       "      <th colspan=\"3\" halign=\"left\">numb. of unique sentence auios</th>\n",
       "      <th colspan=\"3\" halign=\"left\">avg audio length (s)</th>\n",
       "      <th colspan=\"3\" halign=\"left\">avg numb. of words per audio</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>dev</th>\n",
       "      <th>test</th>\n",
       "      <th>train</th>\n",
       "      <th>dev</th>\n",
       "      <th>test</th>\n",
       "      <th>train</th>\n",
       "      <th>dev</th>\n",
       "      <th>test</th>\n",
       "      <th>train</th>\n",
       "      <th>dev</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fr</th>\n",
       "      <td>207372</td>\n",
       "      <td>14760</td>\n",
       "      <td>14760</td>\n",
       "      <td>130602</td>\n",
       "      <td>14760</td>\n",
       "      <td>14760</td>\n",
       "      <td>4.544324</td>\n",
       "      <td>5.260833</td>\n",
       "      <td>5.639743</td>\n",
       "      <td>8.898622</td>\n",
       "      <td>9.348848</td>\n",
       "      <td>9.544986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>de</th>\n",
       "      <td>127824</td>\n",
       "      <td>13511</td>\n",
       "      <td>13511</td>\n",
       "      <td>71831</td>\n",
       "      <td>13511</td>\n",
       "      <td>13511</td>\n",
       "      <td>5.146449</td>\n",
       "      <td>5.459307</td>\n",
       "      <td>5.697595</td>\n",
       "      <td>8.689651</td>\n",
       "      <td>8.885871</td>\n",
       "      <td>8.779143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>31698</td>\n",
       "      <td>8940</td>\n",
       "      <td>8951</td>\n",
       "      <td>19387</td>\n",
       "      <td>8940</td>\n",
       "      <td>8951</td>\n",
       "      <td>4.976882</td>\n",
       "      <td>5.734083</td>\n",
       "      <td>6.148922</td>\n",
       "      <td>9.669443</td>\n",
       "      <td>9.718009</td>\n",
       "      <td>9.802257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pt</th>\n",
       "      <td>9158</td>\n",
       "      <td>3318</td>\n",
       "      <td>4023</td>\n",
       "      <td>6014</td>\n",
       "      <td>3318</td>\n",
       "      <td>4023</td>\n",
       "      <td>4.000819</td>\n",
       "      <td>4.726160</td>\n",
       "      <td>4.722993</td>\n",
       "      <td>7.348766</td>\n",
       "      <td>7.623568</td>\n",
       "      <td>7.890132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>es</th>\n",
       "      <td>79013</td>\n",
       "      <td>13221</td>\n",
       "      <td>13221</td>\n",
       "      <td>64351</td>\n",
       "      <td>13221</td>\n",
       "      <td>13221</td>\n",
       "      <td>5.109949</td>\n",
       "      <td>5.896785</td>\n",
       "      <td>6.140504</td>\n",
       "      <td>9.458089</td>\n",
       "      <td>9.798200</td>\n",
       "      <td>9.863324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nl</th>\n",
       "      <td>7108</td>\n",
       "      <td>1699</td>\n",
       "      <td>1699</td>\n",
       "      <td>1893</td>\n",
       "      <td>1699</td>\n",
       "      <td>1699</td>\n",
       "      <td>3.661269</td>\n",
       "      <td>3.978693</td>\n",
       "      <td>4.264626</td>\n",
       "      <td>8.137029</td>\n",
       "      <td>8.331371</td>\n",
       "      <td>8.398470</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   numb. of audios               numb. of unique sentence auios                \\\n",
       "             train    dev   test                          train    dev   test   \n",
       "fr          207372  14760  14760                         130602  14760  14760   \n",
       "de          127824  13511  13511                          71831  13511  13511   \n",
       "it           31698   8940   8951                          19387   8940   8951   \n",
       "pt            9158   3318   4023                           6014   3318   4023   \n",
       "es           79013  13221  13221                          64351  13221  13221   \n",
       "nl            7108   1699   1699                           1893   1699   1699   \n",
       "\n",
       "   avg audio length (s)                     avg numb. of words per audio  \\\n",
       "                  train       dev      test                        train   \n",
       "fr             4.544324  5.260833  5.639743                     8.898622   \n",
       "de             5.146449  5.459307  5.697595                     8.689651   \n",
       "it             4.976882  5.734083  6.148922                     9.669443   \n",
       "pt             4.000819  4.726160  4.722993                     7.348766   \n",
       "es             5.109949  5.896785  6.140504                     9.458089   \n",
       "nl             3.661269  3.978693  4.264626                     8.137029   \n",
       "\n",
       "                        \n",
       "         dev      test  \n",
       "fr  9.348848  9.544986  \n",
       "de  8.885871  8.779143  \n",
       "it  9.718009  9.802257  \n",
       "pt  7.623568  7.890132  \n",
       "es  9.798200  9.863324  \n",
       "nl  8.331371  8.398470  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import librosa\n",
    "# import numpy as np\n",
    "\n",
    "# X_en_stat = pd.DataFrame(columns=[['numb. of audios','numb. of audios','numb. of audios',\n",
    "#                                    'numb. of unique sentence auios','numb. of unique sentence auios','numb. of unique sentence auios',\n",
    "#                                    'avg audio length (s)','avg audio length (s)','avg audio length (s)',\n",
    "#                                    'avg numb. of words per audio','avg numb. of words per audio','avg numb. of words per audio'],\n",
    "#                                  ['train','dev','test',\n",
    "#                                   'train','dev','test',\n",
    "#                                   'train','dev','test',\n",
    "#                                   'train', 'dev','test']])\n",
    "\n",
    "# for lang in XX_EN_LANGUAGES:\n",
    "#     SRC_AUDIO_DIR = COVOST_DIR + '/' + lang\n",
    "#     audiodir = SRC_AUDIO_DIR + '/clips'\n",
    "#     TRANSLATIONS_DIR =  COVOST_DIR + '/covost2' + f'/{lang}_en'\n",
    "#     for split in ['train', 'dev', 'test']:\n",
    "#         split_df = read_tsv_split(TRANSLATIONS_DIR, src_lang=lang, tgt_lang='en', split=split, audiodir=audiodir)\n",
    "#         X_en_stat.at[lang, ('numb. of audios', split)] = len(split_df)\n",
    "#         X_en_stat.at[lang, ('numb. of unique sentence auios', split)] = len(set(split_df['sentence'].values))\n",
    "#         X_en_stat.at[lang, ('avg audio length (s)', split)] = np.mean(np.array([librosa.get_duration(filename=audiodir + '/' + path) for path in split_df['path'].values]))\n",
    "#         X_en_stat.at[lang, ('avg numb. of words per audio', split)] = np.mean(np.array([len(s.split()) for s in split_df['sentence'].values]))\n",
    "\n",
    "# X_en_stat.to_csv(COVOST_DIR + '/' + 'X_en_stat.csv')\n",
    "\n",
    "X_en_stat = pd.read_csv(COVOST_DIR + '/' + 'X_en_stat.csv', index_col=0, header=[0,1])\n",
    "X_en_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">numb. of audios</th>\n",
       "      <th colspan=\"3\" halign=\"left\">numb. of unique sentence auios</th>\n",
       "      <th colspan=\"3\" halign=\"left\">avg audio length (s)</th>\n",
       "      <th colspan=\"3\" halign=\"left\">avg numb. of words per audio</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>dev</th>\n",
       "      <th>test</th>\n",
       "      <th>train</th>\n",
       "      <th>dev</th>\n",
       "      <th>test</th>\n",
       "      <th>train</th>\n",
       "      <th>dev</th>\n",
       "      <th>test</th>\n",
       "      <th>train</th>\n",
       "      <th>dev</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>de</th>\n",
       "      <td>289413</td>\n",
       "      <td>15531</td>\n",
       "      <td>15531</td>\n",
       "      <td>232958</td>\n",
       "      <td>15531</td>\n",
       "      <td>15531</td>\n",
       "      <td>5.300589</td>\n",
       "      <td>6.006085</td>\n",
       "      <td>5.670569</td>\n",
       "      <td>9.801999</td>\n",
       "      <td>9.852295</td>\n",
       "      <td>9.100315</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   numb. of audios               numb. of unique sentence auios                \\\n",
       "             train    dev   test                          train    dev   test   \n",
       "de          289413  15531  15531                         232958  15531  15531   \n",
       "\n",
       "   avg audio length (s)                     avg numb. of words per audio  \\\n",
       "                  train       dev      test                        train   \n",
       "de             5.300589  6.006085  5.670569                     9.801999   \n",
       "\n",
       "                        \n",
       "         dev      test  \n",
       "de  9.852295  9.100315  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# en_X_stat = pd.DataFrame(columns=[['numb. of audios','numb. of audios','numb. of audios',\n",
    "#                                    'numb. of unique sentence auios','numb. of unique sentence auios','numb. of unique sentence auios',\n",
    "#                                    'avg audio length (s)','avg audio length (s)','avg audio length (s)',\n",
    "#                                    'avg numb. of words per audio','avg numb. of words per audio','avg numb. of words per audio'],\n",
    "#                                  ['train','dev','test',\n",
    "#                                   'train','dev','test',\n",
    "#                                   'train','dev','test',\n",
    "#                                   'train', 'dev','test']])\n",
    "\n",
    "# for lang in EN_XX_LANGUAGES:\n",
    "#     SRC_AUDIO_DIR = COVOST_DIR + '/' + 'en'\n",
    "#     audiodir = SRC_AUDIO_DIR + '/clips'\n",
    "#     TRANSLATIONS_DIR =  COVOST_DIR + '/covost2' + f'/en_{lang}'\n",
    "#     for split in ['train', 'dev', 'test']:\n",
    "#         split_df = read_tsv_split(TRANSLATIONS_DIR, src_lang='en', tgt_lang=lang, split=split, audiodir=audiodir)\n",
    "#         en_X_stat.at[lang, ('numb. of audios', split)] = len(split_df)\n",
    "#         en_X_stat.at[lang, ('numb. of unique sentence auios', split)] = len(set(split_df['sentence'].values))\n",
    "#         en_X_stat.at[lang, ('avg audio length (s)', split)] = np.mean(np.array([librosa.get_duration(filename=audiodir + '/' + path) for path in split_df['path'].values]))\n",
    "#         en_X_stat.at[lang, ('avg numb. of words per audio', split)] = np.mean(np.array([len(s.split()) for s in split_df['sentence'].values]))\n",
    "\n",
    "# en_X_stat.to_csv(COVOST_DIR + '/' + 'en_X_stat.csv')        \n",
    "\n",
    "en_X_stat = pd.read_csv(COVOST_DIR + '/' + 'en_X_stat.csv', index_col=0, header=[0,1])\n",
    "en_X_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess a sample dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[WinError 183] Cannot create a file when that file already exists: 'data/CoVoST2/preprocessed/dummy/en_de'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-6445e514997b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtgt_lang\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'de'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mpreprocessed_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf'preprocessed/dummy/{src_lang}_{tgt_lang}'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'{COVOST_DIR}/{preprocessed_dir}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mSRC_AUDIO_DIR\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCOVOST_DIR\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msrc_lang\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileExistsError\u001b[0m: [WinError 183] Cannot create a file when that file already exists: 'data/CoVoST2/preprocessed/dummy/en_de'"
     ]
    }
   ],
   "source": [
    "# Location to save the preprocessed data\n",
    "src_lang = 'en'\n",
    "tgt_lang = 'de'\n",
    "preprocessed_dir = f'preprocessed/{src_lang}_{tgt_lang}'\n",
    "os.mkdir(f'{COVOST_DIR}/{preprocessed_dir}')\n",
    "\n",
    "SRC_AUDIO_DIR = COVOST_DIR + '/' + src_lang\n",
    "audiodir = SRC_AUDIO_DIR + '/clips'\n",
    "\n",
    "TRANSLATIONS_DIR =  COVOST_DIR + '/covost2' + f'/{src_lang}_{tgt_lang}'\n",
    "train_df = read_tsv_split(TRANSLATIONS_DIR, src_lang=src_lang, tgt_lang=tgt_lang, split='train', audiodir=audiodir)\n",
    "val_df = read_tsv_split(TRANSLATIONS_DIR, src_lang=src_lang, tgt_lang=tgt_lang, split='dev', audiodir=audiodir)\n",
    "test_df = read_tsv_split(TRANSLATIONS_DIR, src_lang=src_lang, tgt_lang=tgt_lang, split='test', audiodir=audiodir)\n",
    "\n",
    "train_audios_list = [audiodir + '/' + path for path in train_df['path']]\n",
    "val_audios_list = [audiodir + '/' + path for path in val_df['path']]\n",
    "test_audios_list = [audiodir + '/' + path for path in test_df['path']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess the audios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since librosa does not deal with `.mp3` files, we need a wrapper function to load `.mp3` audios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "# Create a .wav verson of a .mp3 file in the same location, and return the path to the .wav file\n",
    "def mp3_to_wav(mp3_path):\n",
    "    wav_path = f\"{mp3_path[:-4]}.wav\"\n",
    "    gf = os.system(f\"\"\"ffmpeg -i {mp3_path} {wav_path}\"\"\")\n",
    "    return wav_path\n",
    "    \n",
    "# Wrapper function to load .mp3 audio\n",
    "def load_mp3(mp3_path, sr=22050, mono=True, offset=0.0, duration=None, dtype=np.float32, res_type='kaiser_best'):\n",
    "    wav_path = mp3_to_wav(mp3_path)\n",
    "    signal, sample_rate = librosa.load(wav_path, sr, mono, offset, duration, dtype, res_type)\n",
    "    # Remove the .wav file when we're done\n",
    "    os.remove(wav_path)\n",
    "    return signal, sample_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the MFCC features of the audios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from python_speech_features import logfbank, calculate_delta, normalize\n",
    "from kaldiio import WriteHelper\n",
    "\n",
    "def preprocess_audios(audio_paths, output_file_prefix):\n",
    "    out_ark = output_file_prefix + \".ark\"\n",
    "    out_scp = output_file_prefix + \".scp\"\n",
    "    count=0\n",
    "\n",
    "    with WriteHelper('ark,scp:'+out_ark+','+out_scp) as writer:\n",
    "        for audio in audio_paths:\n",
    "            if audio.endswith('.mp3'):\n",
    "                signal, sample_rate = load_mp3(audio, sr=16000)\n",
    "            else:\n",
    "                signal, sample_rate = librosa.load(audio, sr=16000)\n",
    "            logmel = logfbank(signal, samplerate=sample_rate)\n",
    "            delta = calculate_delta(logmel)\n",
    "            features = np.concatenate([logmel, delta], axis=1)\n",
    "            features = normalize(features) # features.shape gives (x, 80)\n",
    "            writer(str(count), features)\n",
    "            count = count + 1\n",
    "    return out_ark, out_scp\n",
    "            \n",
    "preprocess_audios(train_audios_list, f'{COVOST_DIR}/{preprocessed_dir}/{src_lang}_audio_train')\n",
    "preprocess_audios(val_audios_list, f'{COVOST_DIR}/{preprocessed_dir}/{src_lang}_audio_val')\n",
    "preprocess_audios(test_audios_list, f'{COVOST_DIR}/{preprocessed_dir}/{src_lang}_audio_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a text file with one-sentence-per-line from the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_transcription(info_df, audio_paths, output_file_prefix, transcription_type='translated'):\n",
    "    \"\"\" transcription_type is either 'original' or 'translated'\n",
    "    \"\"\"\n",
    "    info_df_re_indexed = info_df.set_index('path')\n",
    "    with open(f\"{output_file_prefix}.txt\", \"w\", encoding=\"utf-8\") as out_file:\n",
    "        for audio_path in audio_paths:\n",
    "            # write line to output file\n",
    "            audio_name = os.path.basename(audio_path)\n",
    "            if transcription_type == 'original':\n",
    "                out_file.write(prepare_sentence(info_df_re_indexed.loc[audio_name]['sentence']))\n",
    "            elif transcription_type == 'translated':\n",
    "                out_file.write(prepare_sentence(info_df_re_indexed.loc[audio_name]['translation']))\n",
    "            else:\n",
    "                raise RuntimeError(\"transcription_type is either 'original' or 'translated'\")\n",
    "            out_file.write(\"\\n\")\n",
    "    return f\"{output_file_prefix}.txt\"\n",
    "\n",
    "def prepare_sentence(sentence):\n",
    "    if sentence.startswith('\"') and sentence.endswith('\"'):\n",
    "        return sentence[1:-1]\n",
    "    return sentence\n",
    "             \n",
    "raw_text_train_path = collect_transcription(train_df, train_audios_list, f'{COVOST_DIR}/{preprocessed_dir}/{tgt_lang}_raw_text_train')\n",
    "raw_text_val_path = collect_transcription(val_df, val_audios_list, f'{COVOST_DIR}/{preprocessed_dir}/{tgt_lang}_raw_text_val')\n",
    "raw_text_test_path = collect_transcription(test_df, test_audios_list, f'{COVOST_DIR}/{preprocessed_dir}/{tgt_lang}_raw_text_test')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use [subword units](https://github.com/google/sentencepiece) to preprocess the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subword_unit(model_path, raw_text_file, output_file, output_type=str):\n",
    "    \"\"\"\n",
    "    Use a Sentence Piece model to do subword unit on a text file\n",
    "    \"\"\"\n",
    "    sp = spm.SentencePieceProcessor(model_file=model_path)\n",
    "    with open(raw_text_file, 'r', encoding=\"utf-8\") as f:\n",
    "        raw_lines = f.readlines()\n",
    "    processed_lines = [' '.join([str(elem) for elem in sp.encode(line, out_type=output_type)]) for line in raw_lines]\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for line in processed_lines:\n",
    "            f.write(line)\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_transcription(transcription_type, train_df, val_df, test_df, train_audios_list, val_audios_list, test_audios_list,\n",
    "                    save_location, lang):\n",
    "    raw_text_train_path = collect_transcription(train_df, train_audios_list, f'{save_location}/{lang}_raw_text_train', transcription_type)\n",
    "    raw_text_val_path = collect_transcription(val_df, val_audios_list, f'{save_location}/{lang}_raw_text_val', transcription_type)\n",
    "    raw_text_test_path = collect_transcription(test_df, test_audios_list, f'{save_location}/{lang}_raw_text_test', transcription_type)\n",
    "\n",
    "    # Train the model to do subword unit on the text\n",
    "    input_file = raw_text_train_path  # one-sentence-per-line raw corpus file\n",
    "    model_prefix = f'{save_location}/{lang}_text'\n",
    "    vocab_size = 8000  # 8000, 16000, or 32000\n",
    "    if lang == 'zh-CN' or lang == 'ja':\n",
    "        character_coverage = 0.9995  # 0.9995 for languages with rich character set like Japanese or Chinese\n",
    "    else:\n",
    "        character_coverage = 1  # and 1.0 for other languages with small character set\n",
    "    model_type = 'unigram'\n",
    "    spm.SentencePieceTrainer.train(input=input_file, model_prefix=model_prefix, vocab_size=vocab_size,\n",
    "                                   character_coverage=character_coverage, model_type=model_type)\n",
    "    \n",
    "    subword_unit(f\"{model_prefix}.model\", raw_text_train_path,\n",
    "                    f'{save_location}/{lang}_text_train.txt')\n",
    "    subword_unit(f\"{model_prefix}.model\", raw_text_val_path,\n",
    "                    f'{save_location}/{lang}_text_val.txt')\n",
    "    subword_unit(f\"{model_prefix}.model\", raw_text_test_path,\n",
    "                    f'{save_location}/{lang}_text_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "preprocess_transcription('original', train_df, val_df, test_df, train_audios_list, val_audios_list, test_audios_list, \n",
    "                         f'{COVOST_DIR}/{preprocessed_dir}', src_lang)\n",
    "preprocess_transcription('translated', train_df, val_df, test_df, train_audios_list, val_audios_list, test_audios_list, \n",
    "                         f'{COVOST_DIR}/{preprocessed_dir}', tgt_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confirm that the duplicated sentences are distributed equally over the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/CoVoST2/covost2/en_de/covost_v2.en_de.train.tsv\", sep='\\t', header=0, encoding=\"utf-8\", escapechar=\"\\\\\", quoting=csv.QUOTE_NONE, na_filter=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(289430, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>sentence</th>\n",
       "      <th>translation</th>\n",
       "      <th>client_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>common_voice_en_19664034.mp3</td>\n",
       "      <td>\"These data components in turn serve as the \"\"...</td>\n",
       "      <td>Diese Datenkomponenten wiederum dienen als die...</td>\n",
       "      <td>4f29be8fe932d773576dd3df5e111929f4e22242232245...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>common_voice_en_19664035.mp3</td>\n",
       "      <td>The church is unrelated to the Jewish politica...</td>\n",
       "      <td>Die Kirche ist nicht mit der jüdischen politis...</td>\n",
       "      <td>4f29be8fe932d773576dd3df5e111929f4e22242232245...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>common_voice_en_19664037.mp3</td>\n",
       "      <td>The following represents architectures which h...</td>\n",
       "      <td>Die folgenden Architekturen sind stellvertrete...</td>\n",
       "      <td>4f29be8fe932d773576dd3df5e111929f4e22242232245...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>common_voice_en_19664038.mp3</td>\n",
       "      <td>Additionally, the pulse output can be directed...</td>\n",
       "      <td>Außerdem kann die Impulsausgabe durch eine von...</td>\n",
       "      <td>4f29be8fe932d773576dd3df5e111929f4e22242232245...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>common_voice_en_19664040.mp3</td>\n",
       "      <td>The two are robbed by a pickpocket who is losi...</td>\n",
       "      <td>Die Zwei werden von einem Taschendieb ausgerau...</td>\n",
       "      <td>4f29be8fe932d773576dd3df5e111929f4e22242232245...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           path  \\\n",
       "0  common_voice_en_19664034.mp3   \n",
       "1  common_voice_en_19664035.mp3   \n",
       "2  common_voice_en_19664037.mp3   \n",
       "3  common_voice_en_19664038.mp3   \n",
       "4  common_voice_en_19664040.mp3   \n",
       "\n",
       "                                            sentence  \\\n",
       "0  \"These data components in turn serve as the \"\"...   \n",
       "1  The church is unrelated to the Jewish politica...   \n",
       "2  The following represents architectures which h...   \n",
       "3  Additionally, the pulse output can be directed...   \n",
       "4  The two are robbed by a pickpocket who is losi...   \n",
       "\n",
       "                                         translation  \\\n",
       "0  Diese Datenkomponenten wiederum dienen als die...   \n",
       "1  Die Kirche ist nicht mit der jüdischen politis...   \n",
       "2  Die folgenden Architekturen sind stellvertrete...   \n",
       "3  Außerdem kann die Impulsausgabe durch eine von...   \n",
       "4  Die Zwei werden von einem Taschendieb ausgerau...   \n",
       "\n",
       "                                           client_id  \n",
       "0  4f29be8fe932d773576dd3df5e111929f4e22242232245...  \n",
       "1  4f29be8fe932d773576dd3df5e111929f4e22242232245...  \n",
       "2  4f29be8fe932d773576dd3df5e111929f4e22242232245...  \n",
       "3  4f29be8fe932d773576dd3df5e111929f4e22242232245...  \n",
       "4  4f29be8fe932d773576dd3df5e111929f4e22242232245...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "232975"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentence'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75563, 4)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dup = df[df.duplicated(subset=['sentence'], keep=False)]\n",
    "dup.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>sentence</th>\n",
       "      <th>translation</th>\n",
       "      <th>client_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>common_voice_en_589230.mp3</td>\n",
       "      <td>What did you think of that trip.</td>\n",
       "      <td>Wie hat dir die Reise gefallen?</td>\n",
       "      <td>4f9d2db67e38d0513bf84559689e88b6e790b7ed82d930...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>common_voice_en_589234.mp3</td>\n",
       "      <td>Two men spoke in Arabic while having a drink a...</td>\n",
       "      <td>Zwei Männer, die an der Bar etwas tranken, spr...</td>\n",
       "      <td>4f9d2db67e38d0513bf84559689e88b6e790b7ed82d930...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>common_voice_en_181712.mp3</td>\n",
       "      <td>Two children playing on a statue</td>\n",
       "      <td>Zwei Kinder spielen auf einer Statue.</td>\n",
       "      <td>4fdb7a7c4e6b6e03754daa018cb1af74b93024a58502b3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>common_voice_en_19643733.mp3</td>\n",
       "      <td>He was still charming, venerable, and courteou...</td>\n",
       "      <td>Er war noch immer charmant, ehrwürdig, zuvorko...</td>\n",
       "      <td>4ffcdeb024dca515637e87338a67b9e9ea69483761a461...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>common_voice_en_19643737.mp3</td>\n",
       "      <td>\"All five of the \"\"general electorates\"\" were ...</td>\n",
       "      <td>Die General Voters Party gewann alle fünf der ...</td>\n",
       "      <td>4ffcdeb024dca515637e87338a67b9e9ea69483761a461...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28930</th>\n",
       "      <td>common_voice_en_18608834.mp3</td>\n",
       "      <td>‘No, no!’ said the Queen.</td>\n",
       "      <td>Die Königin sagte: ‘Nein, nein!‘</td>\n",
       "      <td>fc2591bf6268fc853f1e782a156d49b6408449eef9588d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28931</th>\n",
       "      <td>common_voice_en_18608835.mp3</td>\n",
       "      <td>‘Yes,’ said Alice, ‘we learned French and music.’</td>\n",
       "      <td>‘Ja,‘ sagte Alice, ‘wir haben für Mathe und Fr...</td>\n",
       "      <td>fc2591bf6268fc853f1e782a156d49b6408449eef9588d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28932</th>\n",
       "      <td>common_voice_en_18608836.mp3</td>\n",
       "      <td>Do you hear any sound?</td>\n",
       "      <td>Hörst du etwas?</td>\n",
       "      <td>fc2591bf6268fc853f1e782a156d49b6408449eef9588d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28941</th>\n",
       "      <td>common_voice_en_19645214.mp3</td>\n",
       "      <td>McGonigal was born in Melville, Saskatchewan.</td>\n",
       "      <td>McGonigal wurde in Melville, Saskatchewan gebo...</td>\n",
       "      <td>fcd1914648b65b2b9eb150d55ed6381df311512bf31d40...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28943</th>\n",
       "      <td>common_voice_en_18242508.mp3</td>\n",
       "      <td>\"Let's pretend everything is prepped.\"</td>\n",
       "      <td>Lass uns so tun, als wäre alles vorbereitet.</td>\n",
       "      <td>fda2190bf27d7211f81aef96545309e6a32cff8681d31a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6955 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               path  \\\n",
       "10       common_voice_en_589230.mp3   \n",
       "11       common_voice_en_589234.mp3   \n",
       "28       common_voice_en_181712.mp3   \n",
       "42     common_voice_en_19643733.mp3   \n",
       "46     common_voice_en_19643737.mp3   \n",
       "...                             ...   \n",
       "28930  common_voice_en_18608834.mp3   \n",
       "28931  common_voice_en_18608835.mp3   \n",
       "28932  common_voice_en_18608836.mp3   \n",
       "28941  common_voice_en_19645214.mp3   \n",
       "28943  common_voice_en_18242508.mp3   \n",
       "\n",
       "                                                sentence  \\\n",
       "10                      What did you think of that trip.   \n",
       "11     Two men spoke in Arabic while having a drink a...   \n",
       "28                      Two children playing on a statue   \n",
       "42     He was still charming, venerable, and courteou...   \n",
       "46     \"All five of the \"\"general electorates\"\" were ...   \n",
       "...                                                  ...   \n",
       "28930                          ‘No, no!’ said the Queen.   \n",
       "28931  ‘Yes,’ said Alice, ‘we learned French and music.’   \n",
       "28932                             Do you hear any sound?   \n",
       "28941      McGonigal was born in Melville, Saskatchewan.   \n",
       "28943             \"Let's pretend everything is prepped.\"   \n",
       "\n",
       "                                             translation  \\\n",
       "10                       Wie hat dir die Reise gefallen?   \n",
       "11     Zwei Männer, die an der Bar etwas tranken, spr...   \n",
       "28                 Zwei Kinder spielen auf einer Statue.   \n",
       "42     Er war noch immer charmant, ehrwürdig, zuvorko...   \n",
       "46     Die General Voters Party gewann alle fünf der ...   \n",
       "...                                                  ...   \n",
       "28930                   Die Königin sagte: ‘Nein, nein!‘   \n",
       "28931  ‘Ja,‘ sagte Alice, ‘wir haben für Mathe und Fr...   \n",
       "28932                                    Hörst du etwas?   \n",
       "28941  McGonigal wurde in Melville, Saskatchewan gebo...   \n",
       "28943       Lass uns so tun, als wäre alles vorbereitet.   \n",
       "\n",
       "                                               client_id  \n",
       "10     4f9d2db67e38d0513bf84559689e88b6e790b7ed82d930...  \n",
       "11     4f9d2db67e38d0513bf84559689e88b6e790b7ed82d930...  \n",
       "28     4fdb7a7c4e6b6e03754daa018cb1af74b93024a58502b3...  \n",
       "42     4ffcdeb024dca515637e87338a67b9e9ea69483761a461...  \n",
       "46     4ffcdeb024dca515637e87338a67b9e9ea69483761a461...  \n",
       "...                                                  ...  \n",
       "28930  fc2591bf6268fc853f1e782a156d49b6408449eef9588d...  \n",
       "28931  fc2591bf6268fc853f1e782a156d49b6408449eef9588d...  \n",
       "28932  fc2591bf6268fc853f1e782a156d49b6408449eef9588d...  \n",
       "28941  fcd1914648b65b2b9eb150d55ed6381df311512bf31d40...  \n",
       "28943  fda2190bf27d7211f81aef96545309e6a32cff8681d31a...  \n",
       "\n",
       "[6955 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dup.loc[:round(289430/10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observe**: the duplicated sentences are distributed equally throughout the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create artificial language: reversed English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dlrow olleh!'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "def reverse_sentence(line):\n",
    "    reverse_line = line[::-1]\n",
    "    reverse_line = reverse_line.lower()\n",
    "    reverse_line = reverse_line.translate(str.maketrans('', '', string.punctuation))\n",
    "    if line[-1] in string.punctuation:\n",
    "        # Put back the last punctuation of the sentence if any\n",
    "        reverse_line = reverse_line + line[-1]\n",
    "    # Capitalize the beginning of the sentence\n",
    "    reverse_line = reverse_line[0].upper() + reverse_line[1:]\n",
    "    return reverse_line\n",
    "     \n",
    "reverse_sentence(\"Hello World!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"She'll be all right.\", \"All's well that ends well.\"]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'enr_raw_text_test.txt'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reverse_transcription(original_file, output_file):\n",
    "    with open(original_file, 'r', encoding=\"utf-8\") as f:\n",
    "        original_sentences = f.readlines()\n",
    "    original_sentences = [sentence.rstrip('\\n') for sentence in original_sentences]\n",
    "    print(original_sentences[:2])\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as out_file:\n",
    "        for sentence in original_sentences:\n",
    "            out_file.write(reverse_sentence(sentence))\n",
    "            out_file.write(\"\\n\")\n",
    "    return output_file\n",
    "\n",
    "reverse_transcription(\"en_raw_text_test.txt\", \"enr_raw_text_test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_reversed_transcription(raw_text_train_path, raw_text_val_path, raw_text_test_path, save_location, lang):\n",
    "    # Train the model to do subword unit on the text\n",
    "    input_file = raw_text_train_path  # one-sentence-per-line raw corpus file\n",
    "    model_prefix = f'{save_location}/{lang}_text'\n",
    "    # TODO\n",
    "    vocab_size = 8000  # 8000, 16000, or 32000\n",
    "    if lang == 'zh-CN' or lang == 'ja':\n",
    "        character_coverage = 0.9995  # 0.9995 for languages with rich character set like Japanese or Chinese\n",
    "    else:\n",
    "        character_coverage = 1  # and 1.0 for other languages with small character set\n",
    "    model_type = 'unigram'\n",
    "    spm.SentencePieceTrainer.train(input=input_file, model_prefix=model_prefix, vocab_size=vocab_size,\n",
    "                                   character_coverage=character_coverage, model_type=model_type)\n",
    "\n",
    "    subword_unit(f\"{model_prefix}.model\", raw_text_train_path,\n",
    "                 f'{save_location}/{lang}_text_train.txt')\n",
    "    subword_unit(f\"{model_prefix}.model\", raw_text_val_path,\n",
    "                 f'{save_location}/{lang}_text_val.txt')\n",
    "    subword_unit(f\"{model_prefix}.model\", raw_text_test_path,\n",
    "                 f'{save_location}/{lang}_text_test.txt')\n",
    "    \n",
    "def subword_unit(model_path, raw_text_file, output_file, output_type=str):\n",
    "    \"\"\"\n",
    "    Use a Sentence Piece model to do subword unit on a text file\n",
    "    \"\"\"\n",
    "    sp = spm.SentencePieceProcessor(model_file=model_path)\n",
    "    with open(raw_text_file, 'r', encoding=\"utf-8\") as f:\n",
    "        raw_lines = f.readlines()\n",
    "    processed_lines = [' '.join([str(elem) for elem in sp.encode(line, out_type=output_type)]) for line in raw_lines]\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for line in processed_lines:\n",
    "            f.write(line)\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
